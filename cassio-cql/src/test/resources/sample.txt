 def main(args: Array[String]): Unit = {
    logger.info("creating spark session.....")
    val applicationConf = ApplicationConfigBuilder(args.toList).build()

    val accessKeyId = applicationConf.awsAccessKey
    val secretKeyId = applicationConf.awsSecretKey

    val catalog = applicationConf.icebergCatalogName
    val hiveMetastoreUri = applicationConf.hiveMetastoreUri
    val catalogWarehousePath = applicationConf.catalogWarehousePath

    implicit val spark: SparkSession = SparkSession
      .builder()
      .appName(applicationConf.datasetName + "_" + applicationConf.env)
      .config("spark.driver.allowMultipleContexts", "true")
      .config("spark.submit.deployMode", "cluster")
      .config("spark.dynamicAllocation.enabled", "true")
      .config("spark.shuffle.spill.compress", "true")
      .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
      .config("spark.hadoop.fs.s3a.access.key", accessKeyId)
      .config("spark.hadoop.fs.s3a.secret.key", secretKeyId)
      .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
      .config("spark.hadoop.com.amazonaws.services.s3.enableV4", "true")
      .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .config(s"spark.sql.catalog.$catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
      .config(s"spark.sql.catalog.$catalog.type", "hive")
      .config(s"spark.sql.catalog.$catalog", "org.apache.iceberg.spark.SparkCatalog")
      .config(s"spark.sql.catalog.$catalog.warehouse", s"$catalogWarehousePath")
      .config(s"spark.sql.catalog.$catalog.uri", s"$hiveMetastoreUri")
      .config("spark.files", "/Users/utkarsh2811/work/repo/pw-nebula-core/src/main/resources/secure-connect-alakhai-staging-mumbai.zip")
      .withExtensions(new CassandraSparkExtensions)
      .enableHiveSupport()
      .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")
    if (!applicationConf.isAudit) {
      Audit.isAudit = false
    }

    spark.conf.set("spark.cassandra.connection.config.cloud.path", "secure-connect-alakhai-staging-mumbai.zip")
    spark.conf.set("spark.cassandra.auth.username", "XXXXXXXXXXX")
    spark.conf.set("spark.cassandra.auth.password", "XXXXXXXXXXX")
    spark.conf.set("spark.dse.continuousPagingEnabled", "false")
    spark.conf.set("confirm.truncate", "true")

    val csvDf = spark.read.option("header", "true").csv("s3://data-airflow-production-app/aiguru_qbg_to_cassandra/ingest_date=20240830T090159/*.csv")

    /*
    now from here, i read the csvDf and wish to do below things:
      1. collect and combine few columns from my csvDf
      2. embedd few of these columns as vector data
      3. store it to cassandra

    a sample python code which is doing the job for me:

## makeContentAndMetadata method
    def makeContentAndMetadata(final_df):
    content_list = []
    metadata_list = []
    id_list = []
    docs = []
    for row in final_df.values:
        # print(row)
        content = row[3]
        id = row[0]
        metadata = {
            "source": "qbg_foundation",
            "subject": row[7],
            "chapter": row[8],
            "topic": row[9],
            "subtopic": row[10],
            "type": "ask_doubt_foundation",
            "intent_type": "academic_foundation",
            "text_solution": row[5],
            "video_solution": row[6],
            "question": row[3],
            "id": id,
            "created_at": row[1],
            "updated_at": row[2],
            "data_version": 'v1'
        }
        content_list.append(content)
        metadata_list.append(metadata)
        id_list.append(id)
    return content_list , metadata_list , id_list

## migrateToCassandra method
    def migrateToCassandra(today,content_list,metadata_list, id_list):
    # Stage
    ASTRA_DB_KEYSPACE = getSecrets('ASTRA_DB_KEYSPACE')
    # Input your Astra DB token string, the one starting with "AstraCS:..."
    ASTRA_DB_TOKEN_BASED_PASSWORD = getSecrets('ASTRA_DB_TOKEN_BASED_PASSWORD')
    ASTRA_DB_TOKEN_BASED_USERNAME = getSecrets('ASTRA_DB_TOKEN_BASED_USERNAME')

    putZipFileInWritablePath()
    ASTRA_DB_SECURE_BUNDLE_PATH_STAGING = directory_path + "secure-connect-alakhai-staging-mumbai.zip"

    astraSession = None
    keyspace = ASTRA_DB_KEYSPACE

    cqlMode = 'astra_db'
    session_staging = getCQLSession(ASTRA_DB_TOKEN_BASED_PASSWORD , ASTRA_DB_TOKEN_BASED_USERNAME, astraSession= None, mode=cqlMode, bundle_path = ASTRA_DB_SECURE_BUNDLE_PATH_STAGING )
    # session_stage = getCQLSession(astraSession= None, mode=cqlMode, bundle_path = ASTRA_DB_SECURE_BUNDLE_PATH_STAGE)
    keyspace = getCQLKeyspace(keyspace, ASTRA_DB_KEYSPACE, mode=cqlMode)
    print(f"keyspace: {keyspace}")

    embeddings_text_ada_003_large = AzureOpenAIEmbeddings(
            # azure_deployment = "text-ada-large-gg",
            openai_api_version = "2024-03-01-preview",
            openai_api_type = "azure",
            openai_api_base = getSecrets('openai_api_base'),
            openai_api_key = getSecrets('openai_api_key'),
            chunk_size = 16,
            validate_base_url = True,
            deployment = "text-ada-large-gg"
        )


    vector_store_text_ada_003_large = cassandra_vector_store("qbg_embedding_003_foundation", embeddings_text_ada_003_large, session_staging)

    chunk_size = 16
    bucket_name = 'data-airflow-production-app'
    success_path = f"aiguru_qbg_to_cassandra/{today}/question_to_update_foundation_success.txt"
    failed_path = f"aiguru_qbg_to_cassandra/{today}/question_to_update_foundation_failed.txt"

    success_content = ""
    failed_content = ""

    # Split the data into chunks of size 20000 3424 115392 243952
    for i in range(0, len(content_list), chunk_size):
        print("Iteration :",i)
        chunk_texts = content_list[i:i + chunk_size]
        chunk_metadatas = metadata_list[i:i + chunk_size]
        chunk_ids = id_list[i:i + chunk_size]
        # Call the function for the current chunk
        try:
            data = vector_store_text_ada_003_large.add_texts(texts=chunk_texts, metadatas=chunk_metadatas, ids=chunk_ids)
            success_content += "\n".join(data) + "\n"
        except Exception as e:
            failed_content += "\n".join(chunk_ids) + "\n"

    success_count = len(success_content.strip().split("\n")) if success_content else 0
    failed_count = len(failed_content.strip().split("\n")) if failed_content else 0
    print(f"{success_count} rows migrated successfully")
    print(f"{failed_count} rows failed to migrate")

    upload_to_s3(bucket_name, success_path, success_content)
    if failed_content != "":
        upload_to_s3(bucket_name, failed_path, failed_content)
    return success_count,failed_count

## cassandra_vector_store method
    def cassandra_vector_store(table_name, embeddings, session):
    """
    Temp method to fetch cassandra vector store to comply with metadata filtering
    """
    print(f"intialising new cassandra_vector_store")
    return Cassandra(
        embedding=embeddings,
        session=session,
        keyspace="dev_alakh_ai",
        table_name=table_name,    #sst_response # pw_knowledge_base
    )

    */

}